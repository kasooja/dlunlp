package edu.insight.unlp.nn.rnn;

import java.util.List;

import edu.insight.unlp.nn.ErrorFunction;
import edu.insight.unlp.nn.NN;
import edu.insight.unlp.nn.NNLayer;
import edu.insight.unlp.nn.common.SequenceM21;

/*
 * RNNs
 */
public class RNN implements NN {

	public List<NNLayer> layers;
	public ErrorFunction ef;
	public double[][] networkOutput;

	public RNN(ErrorFunction ef){
		this.ef = ef;
	}

	public double[] output(double[] input) {
		double[] result = input;
		for(NNLayer nnlayer : layers){
			result = nnlayer.output(result);
		}
		return result;
	}

	@Override
	public void update(double learningRate, double momentum) {
		for(NNLayer layer : layers){
			int index = layers.indexOf(layer);
			if(index!=layers.size()-1){
				layers.get(index+1).update(learningRate, momentum);
			}
		}
	}

	@Override
	public double error(double[] inputs, double[] target) {
		return 0;
	}

	public double batchgdTrain(double[][] inputs, double[][] targets, double learningRate, int batchSize, boolean shuffle, double momentum){
		return 0.0;
	}

	public double sgdTrainSeq(List<SequenceM21> training, double learningRate, int batchSize, boolean shuffle, double momentum){
		double overallError = 0.0;
		for(SequenceM21 seq : training){
			double[][] inputSeq = seq.inputSeq;
			double[] target = seq.target;
			ff(inputSeq);
			double[][] eg = new double[networkOutput.length][];
			for(int i=0; i<networkOutput.length; i++){
				eg[i] = ef.error(target, networkOutput[i]);
			}
			double[] bp = bp(eg);
			double error = bp[bp.length-1];
			overallError = overallError + error;
			update(learningRate, momentum);
			resetActivationCounter();
		}
		return overallError / training.size();
	}
	
	public double sgdTrainSeqErrorAtLast(List<SequenceM21> training, double learningRate, int batchSize, boolean shuffle, double momentum){
		double overallError = 0.0;
		for(SequenceM21 seq : training){
			double[][] inputSeq = seq.inputSeq;
			double[] target = seq.target;
			double[] networkOutput = ff(inputSeq);
			double[] eg = ef.error(target, networkOutput);
			eg = bp(eg);
			double error = eg[networkOutput.length];
			overallError = overallError + error;
			update(learningRate, momentum);
			resetActivationCounter();
		}
		return overallError / training.size();
	}

	private double[] ff(double[][] inputSeq){
		double[] activations = null;
		networkOutput = new double[inputSeq.length][];
		int i = 0;
		for(double[] input : inputSeq){
			activations = input;		
			for(NNLayer layer : layers){
				activations = layer.computeActivations(activations);
			}
			networkOutput[i++] = activations;
		}
		return activations;
	}

	private double[] bp(double[][] errorGradient){
		int o = errorGradient[0].length-1;
		double[] stageErrorGradient = null;
		for(int j=layers.get(layers.size()-1).getActivationCounterVal(); j>=0; j--){
			for(int i = layers.size() - 1; i>0; i--){
				stageErrorGradient = layers.get(i).errorGradient(errorGradient[j]);
			}
			double totalError = stageErrorGradient[stageErrorGradient.length-1];
			if(j-1>-1){
				errorGradient[j-1][o] = totalError;
			}
		}
		return stageErrorGradient;
	}

	private double[] bp(double[] errorGradient){
		int o = errorGradient.length;
		for(int j=layers.get(layers.size()-1).getActivationCounterVal(); j>=0; j--){
			for(int i = layers.size() - 1; i>0; i--){
				errorGradient = layers.get(i).errorGradient(errorGradient);
			}
			double totalError = errorGradient[errorGradient.length-1];
			errorGradient = new double[o]; errorGradient[o-1] = totalError;
		}
		return errorGradient;
	}
	
	public void resetActivationCounter(){
		for(NNLayer layer : layers){
			layer.resetActivationCounter();
		}
	}

	@Override
	public int numOutputUnits() {
		return layers.get(layers.size()-1).numNeuronUnits();
	}

	@Override
	public void setLayers(List<edu.insight.unlp.nn.NNLayer> layers) {
		this.layers = layers;
	}

	@Override
	public void initializeNN() {
		for(NNLayer layer : layers){
			int index = layers.indexOf(layer);
			if(index!=layers.size()-1){
				layers.get(index+1).initializeLayer(layer.numNeuronUnits());
			}
		}
		layers.get(0).initializeLayer(-1);
	}

	@Override
	public List<NNLayer> getLayers() {
		return layers;
	}

	@Override
	public double[] outputSequence(double[][] inputSeq) {
		double[] result = null;
		for(double[] input : inputSeq){
			result = input;		
			for(NNLayer layer : layers){
				result = layer.output(result);
			}
		}
		//networkOutput = result;
		return result;
	}

}
